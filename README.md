# DBLP Semantic Data Processing Pipeline v2.1

A modern DBLP data processing pipeline with scheduling, incremental updates, and modular architecture.

## Key Features

- **ğŸš€ Complete Pipeline**: One-stop processing from download to database storage
- **ğŸ”„ Incremental Processing**: Supports incremental updates, avoiding reprocessing existing data  
- **â° Scheduled Tasks**: Configurable timed tasks based on APScheduler
- **ğŸ—ï¸ Modular Architecture**: Independent service components for easy scaling and maintenance
- **ğŸ“Š PostgreSQL Optimized**: Specialized data storage optimized for PostgreSQL
- **âš™ï¸ Environment Configuration**: Manage all settings via .env files
- **ğŸ“¦ UV Project Management**: Modern Python package management with uv
- **ğŸ“ˆ Progress Tracking**: Detailed progress bars and logging
- **ğŸ›¡ï¸ Error Handling**: Comprehensive error handling and retry mechanisms  
- **ğŸ’¾ Batch Processing**: Memory-friendly batch processing mechanisms
- **âœ¨ NEW: Enhanced Time Tracking**: New `create_time`/`update_time` columns with automatic triggers

## é¡¹ç›®æ¶æ„

```
semantic/
â”œâ”€â”€ src/semantic/           # ä¸»è¦æºä»£ç 
â”‚   â”œâ”€â”€ database/          # æ•°æ®åº“ç›¸å…³æ¨¡å—
â”‚   â”‚   â”œâ”€â”€ connection.py  # æ•°æ®åº“è¿æ¥ç®¡ç†
â”‚   â”‚   â””â”€â”€ models.py      # æ•°æ®æ¨¡å‹å’Œä»“åº“
â”‚   â”œâ”€â”€ services/          # ä¸šåŠ¡æœåŠ¡å±‚
â”‚   â”‚   â”œâ”€â”€ dblp_service.py      # DBLPæ•°æ®å¤„ç†æœåŠ¡
â”‚   â”‚   â””â”€â”€ pipeline_service.py  # æ•°æ®ç®¡é“æœåŠ¡
â”‚   â”œâ”€â”€ scheduler/         # ä»»åŠ¡è°ƒåº¦
â”‚   â”‚   â””â”€â”€ scheduler.py   # APSchedulerè°ƒåº¦å™¨
â”‚   â””â”€â”€ utils/            # å·¥å…·æ¨¡å—
â”‚       â””â”€â”€ config.py     # é…ç½®ç®¡ç†
â”œâ”€â”€ scripts/              # è¿è¡Œè„šæœ¬
â”‚   â”œâ”€â”€ run_scheduler.py   # è°ƒåº¦å™¨å¯åŠ¨è„šæœ¬
â”‚   â””â”€â”€ run_pipeline_once.py # å•æ¬¡è¿è¡Œè„šæœ¬
â”œâ”€â”€ config/              # é…ç½®æ–‡ä»¶
â”œâ”€â”€ logs/                # æ—¥å¿—æ–‡ä»¶
â”œâ”€â”€ data/                # æ•°æ®æ–‡ä»¶
â””â”€â”€ external/            # å¤–éƒ¨ä¸‹è½½æ–‡ä»¶
```

## å¿«é€Ÿå¼€å§‹

### 1. ç¯å¢ƒå‡†å¤‡

```bash
# å®‰è£…uv (å¦‚æœè¿˜æ²¡å®‰è£…)
curl -LsSf https://astral.sh/uv/install.sh | sh

# å…‹éš†/ä¸‹è½½é¡¹ç›®åˆ°æœ¬åœ°
cd semantic

# ä½¿ç”¨uvå®‰è£…ä¾èµ–
uv sync
```

### 2. é…ç½®æ•°æ®åº“

å¤åˆ¶ç¯å¢ƒå˜é‡æ¨¡æ¿ï¼š
```bash
cp .env.example .env
```

ç¼–è¾‘ `.env` æ–‡ä»¶ï¼Œå¡«å…¥ä½ çš„PostgreSQLè¿æ¥ä¿¡æ¯ï¼š
```bash
# PostgreSQL Database Configuration
DB_HOST=localhost
DB_PORT=5432
DB_NAME=dblp_semantic
DB_USER=postgres
DB_PASSWORD=your_password

# Processing Configuration
TARGET_VENUES=acl,naacl,emnlp,findings
ENABLE_VENUE_FILTER=true
BATCH_SIZE=10000
LOG_LEVEL=INFO

# Scheduling Configuration  
SCHEDULE_CRON=0 2 * * 1
ENABLE_INCREMENTAL=true
```

### 3. è¿è¡Œç®¡é“

#### å•æ¬¡è¿è¡Œ
```bash
# ä½¿ç”¨è„šæœ¬è¿è¡Œä¸€æ¬¡å®Œæ•´ç®¡é“
./scripts/run_pipeline_once.py

# æˆ–è€…ä½¿ç”¨uvè¿è¡Œ
uv run python scripts/run_pipeline_once.py
```

#### å¯åŠ¨å®šæ—¶è°ƒåº¦å™¨
```bash
# å¯åŠ¨å®šæ—¶è°ƒåº¦å™¨ï¼ˆæŒ‰é…ç½®çš„cronè¡¨è¾¾å¼è¿è¡Œï¼‰
./scripts/run_scheduler.py

# æ‰‹åŠ¨æ‰§è¡Œä¸€æ¬¡ï¼ˆç«‹å³æ‰§è¡Œï¼‰
./scripts/run_scheduler.py --manual

# åˆ—å‡ºæ‰€æœ‰ä»»åŠ¡
./scripts/run_scheduler.py --list-jobs
```

#### ç›´æ¥ä½¿ç”¨Pythonæ¨¡å—
```bash
# è¿›å…¥è™šæ‹Ÿç¯å¢ƒ
uv shell

# ä½¿ç”¨Pythonæ¨¡å—
python -m semantic.scheduler.scheduler --manual
```

## é…ç½®é€‰é¡¹

### æ•°æ®åº“é…ç½®
- `DB_HOST`: PostgreSQLä¸»æœºåœ°å€
- `DB_PORT`: PostgreSQLç«¯å£
- `DB_NAME`: æ•°æ®åº“åç§°
- `DB_USER`: æ•°æ®åº“ç”¨æˆ·å
- `DB_PASSWORD`: æ•°æ®åº“å¯†ç 

### å¤„ç†é…ç½®
- `TARGET_VENUES`: ç›®æ ‡ä¼šè®®åˆ—è¡¨ï¼ˆé€—å·åˆ†éš”ï¼‰
- `ENABLE_VENUE_FILTER`: æ˜¯å¦å¯ç”¨ä¼šè®®ç­›é€‰ï¼ˆtrue/falseï¼‰
- `BATCH_SIZE`: æ‰¹å¤„ç†å¤§å°ï¼ˆé»˜è®¤10000ï¼‰
- `LOG_LEVEL`: æ—¥å¿—çº§åˆ«ï¼ˆINFO/DEBUG/WARNING/ERRORï¼‰

### è°ƒåº¦é…ç½®
- `SCHEDULE_CRON`: Cronè¡¨è¾¾å¼ï¼ˆé»˜è®¤: 0 2 * * 1ï¼Œæ¯å‘¨ä¸€å‡Œæ™¨2ç‚¹ï¼‰
- `MAX_RETRIES`: æœ€å¤§é‡è¯•æ¬¡æ•°ï¼ˆé»˜è®¤3ï¼‰
- `RETRY_DELAY`: é‡è¯•å»¶è¿Ÿç§’æ•°ï¼ˆé»˜è®¤300ï¼‰

### å¢é‡å¤„ç†é…ç½®
- `ENABLE_INCREMENTAL`: æ˜¯å¦å¯ç”¨å¢é‡å¤„ç†ï¼ˆtrue/falseï¼‰
- `INCREMENTAL_CHECK_DAYS`: å¢é‡æ£€æŸ¥å¤©æ•°ï¼ˆé»˜è®¤7ï¼‰

## æ•°æ®åº“è¡¨ç»“æ„

### ä¸»è¦æ•°æ®è¡¨
```sql
CREATE TABLE dblp_papers (
    id SERIAL PRIMARY KEY,
    key VARCHAR(255) UNIQUE NOT NULL,
    title TEXT NOT NULL,
    authors JSONB NOT NULL,
    author_count INTEGER,
    venue VARCHAR(50),
    year VARCHAR(4),
    pages VARCHAR(50),
    ee TEXT,
    booktitle TEXT,
    doi VARCHAR(100),
    created_at TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

### å¤„ç†å…ƒæ•°æ®è¡¨
```sql
CREATE TABLE dblp_processing_meta (
    id SERIAL PRIMARY KEY,
    process_type VARCHAR(50) NOT NULL,
    last_run_time TIMESTAMP NOT NULL,
    status VARCHAR(20) NOT NULL,
    records_processed INTEGER DEFAULT 0,
    records_inserted INTEGER DEFAULT 0,
    records_updated INTEGER DEFAULT 0,
    error_message TEXT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

### è°ƒåº¦å™¨ä½œä¸šè¡¨
```sql
CREATE TABLE scheduler_jobs (
    -- APSchedulerè‡ªåŠ¨åˆ›å»ºçš„è¡¨ç»“æ„
);
```

## å·¥ä½œæµç¨‹

### å¢é‡å¤„ç†æµç¨‹
1. **æ£€æŸ¥ä¸Šæ¬¡è¿è¡Œæ—¶é—´**: ä»`dblp_processing_meta`è¡¨è·å–ä¸Šæ¬¡æˆåŠŸè¿è¡Œæ—¶é—´
2. **å†³å®šå¤„ç†æ¨¡å¼**: æ ¹æ®é…ç½®å’Œæ—¶é—´é—´éš”å†³å®šæ˜¯å…¨é‡è¿˜æ˜¯å¢é‡å¤„ç†
3. **æ•°æ®å‡†å¤‡**: ä¸‹è½½å’Œè§£å‹DBLPæ•°æ®æ–‡ä»¶
4. **å¢é‡è§£æ**: åªå¤„ç†ä¸å­˜åœ¨äºæ•°æ®åº“çš„æ–°è®ºæ–‡
5. **æ‰¹é‡æ›´æ–°**: ä½¿ç”¨UPSERTæ“ä½œæ‰¹é‡æ’å…¥æˆ–æ›´æ–°æ•°æ®
6. **è®°å½•å…ƒæ•°æ®**: è®°å½•æœ¬æ¬¡å¤„ç†çš„ç»Ÿè®¡ä¿¡æ¯

### è°ƒåº¦å™¨å·¥ä½œæµç¨‹
1. **åˆå§‹åŒ–**: è¿æ¥æ•°æ®åº“ï¼Œè®¾ç½®ä½œä¸šå­˜å‚¨
2. **ä½œä¸šæ³¨å†Œ**: æ ¹æ®Cronè¡¨è¾¾å¼æ³¨å†Œå®šæ—¶ä»»åŠ¡
3. **ä»»åŠ¡æ‰§è¡Œ**: åœ¨æŒ‡å®šæ—¶é—´æ‰§è¡Œæ•°æ®ç®¡é“
4. **é”™è¯¯å¤„ç†**: å¤±è´¥æ—¶è‡ªåŠ¨é‡è¯•ï¼Œè®°å½•é”™è¯¯æ—¥å¿—
5. **çŠ¶æ€ç›‘æ§**: ç›‘å¬ä½œä¸šæ‰§è¡ŒçŠ¶æ€ï¼Œç”Ÿæˆæ‰§è¡ŒæŠ¥å‘Š

## ä½¿ç”¨ç¤ºä¾‹

### ç¼–ç¨‹æ¥å£ä½¿ç”¨
```python
from semantic.services.pipeline_service import DataPipelineService
from semantic.utils.config import AppConfig
from semantic.database.connection import get_db_manager

# åˆ›å»ºé…ç½®
config = AppConfig.from_env()

# åˆ›å»ºç®¡é“æœåŠ¡
pipeline = DataPipelineService(config)

# è¿è¡Œå®Œæ•´ç®¡é“
success = pipeline.run_pipeline()

# å¯¼å‡ºæ•°æ®
if success:
    pipeline.export_to_csv("output/papers.csv")
```

### ç‹¬ç«‹æ¨¡å—ä½¿ç”¨
```python
from semantic.services.dblp_service import DBLPService
from semantic.database.models import PaperRepository
from semantic.database.connection import get_db_manager

# ä½¿ç”¨DBLPæœåŠ¡
dblp_service = DBLPService(config)
papers = dblp_service.parse_papers()

# ä½¿ç”¨æ•°æ®åº“ä»“åº“
db_manager = get_db_manager()
paper_repo = PaperRepository(db_manager)
paper_repo.batch_insert_papers(papers)
```

## å¼€å‘æŒ‡å—

### å®‰è£…å¼€å‘ä¾èµ–
```bash
uv sync --dev
```

### ä»£ç æ ¼å¼åŒ–
```bash
uv run black src/
uv run isort src/
```

### ä»£ç æ£€æŸ¥
```bash
uv run flake8 src/
```

### è¿è¡Œæµ‹è¯•
```bash
uv run pytest tests/
```

## æ‰©å±•å’Œé›†æˆ

### æ·»åŠ æ–°çš„æ•°æ®æº
1. åœ¨`src/semantic/services/`ä¸­åˆ›å»ºæ–°çš„æœåŠ¡ç±»
2. å®ç°ç»Ÿä¸€çš„æ•°æ®æ¥å£ï¼ˆç»§æ‰¿åŸºç¡€æœåŠ¡ç±»ï¼‰
3. åœ¨ç®¡é“æœåŠ¡ä¸­é›†æˆæ–°çš„æ•°æ®æº

### æ·»åŠ æ–°çš„æ•°æ®å¤„ç†æ­¥éª¤
1. åœ¨ç®¡é“æœåŠ¡ä¸­æ·»åŠ æ–°çš„æ­¥éª¤æ–¹æ³•
2. æ›´æ–°`run_pipeline()`æ–¹æ³•çš„æ‰§è¡Œæµç¨‹
3. æ·»åŠ ç›¸åº”çš„é…ç½®é€‰é¡¹å’Œé”™è¯¯å¤„ç†

### é›†æˆå…¶ä»–è°ƒåº¦ç³»ç»Ÿ
1. å®ç°è‡ªå®šä¹‰çš„è°ƒåº¦å™¨ç±»
2. ä¿æŒä¸ç°æœ‰ç®¡é“æœåŠ¡çš„æ¥å£å…¼å®¹
3. æä¾›ç›¸åŒçš„ç›‘æ§å’Œæ—¥å¿—åŠŸèƒ½

## ç›‘æ§å’Œæ—¥å¿—

### æ—¥å¿—æ–‡ä»¶
- ç®¡é“æ‰§è¡Œæ—¥å¿—: `logs/dblp_service_YYYYMMDD_HHMMSS.log`
- è°ƒåº¦å™¨æ—¥å¿—: `logs/scheduler_YYYYMMDD.log`
- æ•°æ®åº“æ“ä½œæ—¥å¿—: é›†æˆåœ¨ç®¡é“æ—¥å¿—ä¸­

### ç›‘æ§æŒ‡æ ‡
- å¤„ç†è®ºæ–‡æ•°é‡
- æ–°å¢/æ›´æ–°è®ºæ–‡ç»Ÿè®¡
- æ‰§è¡Œæ—¶é—´
- é”™è¯¯ç‡
- é‡è¯•æ¬¡æ•°

### æ•°æ®åº“ç›‘æ§
```sql
-- æŸ¥çœ‹å¤„ç†å†å²
SELECT * FROM dblp_processing_meta ORDER BY created_at DESC LIMIT 10;

-- æŸ¥çœ‹æ•°æ®ç»Ÿè®¡
SELECT venue, COUNT(*) as count 
FROM dblp_papers 
GROUP BY venue 
ORDER BY count DESC;
```

## æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

**è¿æ¥å¤±è´¥**
- æ£€æŸ¥PostgreSQLæœåŠ¡æ˜¯å¦è¿è¡Œ
- éªŒè¯.envæ–‡ä»¶ä¸­çš„è¿æ¥ä¿¡æ¯
- ç¡®è®¤æ•°æ®åº“å­˜åœ¨ä¸”ç”¨æˆ·æœ‰æƒé™

**ä¸‹è½½å¤±è´¥**
- æ£€æŸ¥ç½‘ç»œè¿æ¥
- éªŒè¯DBLP URLæ˜¯å¦å¯è®¿é—®
- æ£€æŸ¥ç£ç›˜ç©ºé—´

**è°ƒåº¦å™¨å¯åŠ¨å¤±è´¥**
- æ£€æŸ¥Cronè¡¨è¾¾å¼æ ¼å¼
- éªŒè¯æ•°æ®åº“è¿æ¥
- æŸ¥çœ‹è°ƒåº¦å™¨æ—¥å¿—æ–‡ä»¶

**å¢é‡å¤„ç†å¼‚å¸¸**
- æ£€æŸ¥å…ƒæ•°æ®è¡¨æ˜¯å¦å­˜åœ¨
- éªŒè¯å¢é‡é…ç½®å‚æ•°
- æ¸…ç†ä¸´æ—¶æ–‡ä»¶é‡æ–°è¿è¡Œ

### æ€§èƒ½ä¼˜åŒ–å»ºè®®

1. **æ•°æ®åº“ä¼˜åŒ–**
   - å®šæœŸæ‰§è¡ŒVACUUM ANALYZE
   - ç›‘æ§ç´¢å¼•ä½¿ç”¨æƒ…å†µ
   - è°ƒæ•´æ‰¹å¤„ç†å¤§å°

2. **å†…å­˜ä¼˜åŒ–**
   - æ ¹æ®æœåŠ¡å™¨å†…å­˜è°ƒæ•´æ‰¹å¤„ç†å¤§å°
   - ç›‘æ§å†…å­˜ä½¿ç”¨æƒ…å†µ
   - åŠæ—¶æ¸…ç†ä¸´æ—¶æ–‡ä»¶

3. **ç½‘ç»œä¼˜åŒ–**
   - ä½¿ç”¨ç¨³å®šçš„ç½‘ç»œè¿æ¥
   - è€ƒè™‘ä½¿ç”¨ä»£ç†æœåŠ¡å™¨
   - è®¾ç½®åˆé€‚çš„è¶…æ—¶æ—¶é—´

## è®¸å¯è¯

æ­¤é¡¹ç›®éµå¾ªMITè®¸å¯è¯ã€‚

## è´¡çŒ®æŒ‡å—

æ¬¢è¿æäº¤Pull Requestå’ŒIssueï¼è¯·ç¡®ä¿ï¼š

1. ä»£ç ç¬¦åˆé¡¹ç›®çš„æ ¼å¼è§„èŒƒ
2. æ·»åŠ é€‚å½“çš„æµ‹è¯•ç”¨ä¾‹
3. æ›´æ–°ç›¸å…³æ–‡æ¡£
4. éµå¾ªç°æœ‰çš„æ¶æ„æ¨¡å¼

## æ›´æ–°æ—¥å¿—

### v2.0.0
- é‡æ„ä¸ºæ¨¡å—åŒ–æ¶æ„
- æ·»åŠ å¢é‡å¤„ç†åŠŸèƒ½
- é›†æˆAPSchedulerå®šæ—¶è°ƒåº¦
- å¢å¼ºé”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶
- å®Œå–„æ—¥å¿—å’Œç›‘æ§ç³»ç»Ÿ

### v1.0.0
- åŸºç¡€DBLPæ•°æ®å¤„ç†åŠŸèƒ½
- PostgreSQLæ•°æ®å­˜å‚¨
- åŸºæœ¬çš„æ‰¹å¤„ç†æœºåˆ¶